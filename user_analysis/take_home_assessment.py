# -*- coding: utf-8 -*-
"""Take Home Assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ied2JnzZ8Os0sSlkrQEGgMUTiR5acTDe

##### Below is a template for Asana's early career data dcience take-home assessment. Although we encourage candidates to use a similar format as below, feel free to make changes as needed!

### Data Ingestion
"""

# Commented out IPython magic to ensure Python compatibility.
# How to read the data files in Python

import pandas as pd
# %pip install matplotlib
import matplotlib.pyplot as plt
!pip install -U imbalanced-learn
from imblearn.over_sampling import SMOTE
# %pip install sklearn
import sklearn
from sklearn import preprocessing, metrics
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import linear_model
# %pip install numpy
import numpy as np
import statsmodels.api as sm
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from xgboost import XGBClassifier
from numpy import mean
from numpy import std

users = pd.read_csv("https://s3.amazonaws.com/asana-data-interview/takehome_users-intern.csv")
user_engagement = pd.read_csv("https://s3.amazonaws.com/asana-data-interview/takehome_user_engagement-intern.csv")

"""### 1) Calculating Adoption Rate"""

counts_of_user = user_engagement.groupby("user_id")["user_id"].transform(len)
mask = (counts_of_user > 2)
user_engagement = user_engagement[mask]
user_engagement.time_stamp = pd.to_datetime(user_engagement.time_stamp)
users.rename(columns={'object_id':'user_id'}, inplace=True)
users.head()

adopted_dict = {x: False for x in range(1, len(users)+1)} 
from datetime import datetime
adoption_count = 0
grouped_users = user_engagement.groupby('user_id')  # grouping the main user database

df = user_engagement['time_stamp'].sort_values().reset_index(drop = True)
for group in grouped_users:
  user_id = group[0]

  for i, timestamp in enumerate(df):
    if i == len(group[1]['time_stamp']) - 2:  # making sure that the dates selected have 2 dates after it
      break
    start_time = timestamp
    end_time = start_time + pd.Timedelta('7D')  #adding 7 days to the end date
    time_1 = df[i+1]
    time_2 = df[i+2]
    if (time_1 < end_time) and (time_2 < end_time):  #checcking the times of the 2 consecutives dates after selected one
      adoption_count += 1
      adopted_dict[user_id] = True  # creating a dictionary key for each user id and if they are adopted then store value as True
      #print(adopted_dict)
      break


  #df.iloc by each userid
  #if count is greater than 3:
  #add to adopted user count

#print(adopted_dict)
adopted_users = pd.DataFrame(list(adopted_dict.items()), columns=['user_id', 'adopted'])
users = pd.merge(users, adopted_users, on='user_id', how='outer')

amount_of_users = len(users['user_id'])
count = 0
for user in adopted_dict.values():
  if user == True:
    count += 1
print("Amount of Adopted Users: " + str(count))
print("Amount of total users:" + str(amount_of_users))
print(count/amount_of_users)
#print(users)

"""The adoption rate is approximately 18.7%. I found the adoption rate by filtering the users who have more than 3 logins. After that, I created a grouping of user to find out whether they log in more than 3 times.

### 2) Methodology
"""

fig = plt.figure()
x1 = ['GUEST', 'ORG', 'SIGNUP', 'GOOGLE', 'PERSONAL PROJ']
y1 = [(users[users["adopted"] == False]["creation_source"] == 'GUEST_INVITE').sum(),
     (users[users["adopted"] == False]["creation_source"] == 'ORG_INVITE').sum(),
     (users[users["adopted"] == False]["creation_source"] == 'SIGNUP').sum(),
     (users[users["adopted"] == False]["creation_source"] == 'SIGNUP_GOOGLE_AUTH').sum(),
     (users[users["adopted"] == False]["creation_source"] == "PERSONAL_PROJECTS").sum()]
data1 = dict(zip(x1, y1))
plt.bar(list(data1.keys()), list(data1.values()), color='maroon', width = 0.4)
plt.title('Creation Source for Non-Adopted Users')
plt.xlabel('Creation Source')
plt.ylabel('Sum of Non-Adopted Users Sign Up Method')
plt.show()

fig = plt.figure()
x2 = ['GUEST', 'ORG', 'SIGNUP', 'GOOGLE', 'PERSONAL PROJ']
y2 = [(users[users["adopted"] == True]["creation_source"] == 'GUEST_INVITE').sum(),
     (users[users["adopted"] == True]["creation_source"] == 'ORG_INVITE').sum(),
     (users[users["adopted"] == True]["creation_source"] == 'SIGNUP').sum(),
     (users[users["adopted"] == True]["creation_source"] == 'SIGNUP_GOOGLE_AUTH').sum(),
     (users[users["adopted"] == True]["creation_source"] == "PERSONAL_PROJECTS").sum()]
data2 = dict(zip(x2, y2))
plt.bar(list(data2.keys()), list(data2.values()), color='maroon', width = 0.4)
plt.title('Creation Source for Adopted Users')
plt.xlabel('Creation Source')
plt.ylabel('Sum of Adopted Users Sign Up Method')
plt.show()

"""From the bar graphs above, it is clear that the organization invites are the most popular type of creation for both non-adopted users and adopted users. Now that we know that, I want to move on to figuring out if there are other factors affecting adopted user rate and also to figure out whether organization signups are statistically significant."""

creation_source_dummy_var = pd.get_dummies(users['creation_source'], prefix='creation_source') #adding dummy variables for each creation source
creation_source_dummy_var.head()
weekday_df = users['creation_time'].apply(lambda x : pd.to_datetime(x).weekday())
users['weekday'] = weekday_df.apply(lambda x: 1 if x < 5 else 0)
users['weekday'].head()

users = users.join(creation_source_dummy_var)
#users.describe()

import math
users['invited'] = users['invited_by_user_id'].apply(lambda x: 0 if math.isnan(x) else 1)
users.creation_time = pd.to_datetime(users.creation_time)
users.last_session_creation_time = pd.to_datetime(users.last_session_creation_time, unit='s')
users['creation_month'] = users.creation_time.dt.month
users['last_session_year'] = users.last_session_creation_time.dt.year
users
users.last_session_year.fillna(0, inplace=True)
#print(users['last_session_year'])

predict = "adopted"
ind_vars = ['weekday', 'invited', 'opted_in_to_mailing_list', 'enabled_for_marketing_drip','creation_source_GUEST_INVITE',	'creation_source_ORG_INVITE',	'creation_source_PERSONAL_PROJECTS',	'creation_source_SIGNUP',	'creation_source_SIGNUP_GOOGLE_AUTH']
X = users[ind_vars]
y = users[predict]
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size = 0.1, random_state = 0)
linear = linear_model.LinearRegression()
linear.fit(X_train, y_train) 
acc = linear.score(X_test, y_test) 
print(acc*100)

logit = sm.Logit(y, X)
result = logit.fit()
result.summary()

"""The accuracy of this regression model and r^2 is very low meaning the results may not be as strong as indicated. I am performing logistic regression to figure out the strength of the relation of between all of the potential factors of adoption(listed on the table above) and adoption. In order to improve the R^2 value and the logit regression, I will use SMOTE to try to make the date more balanced."""

oversampling = SMOTE(random_state=0)
X, y = oversampling.fit_resample(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
os_data_X, os_data_y = oversampling.fit_resample(X_train, y_train)
os_data_X = pd.DataFrame(data=os_data_X, columns=X_train.columns)
os_data_y = pd.DataFrame(data=os_data_y, columns=[predict])
print(len(os_data_X))
print(len(os_data_y))

"""##### 2a) Writeup associated with methodology

I decided to compare the sign up Descriptive modeling is the method used for this problem as I want to find out if a user's creation source is likely to indicate if they are an adopted user. Initially, the p-values for all of the creation sources were significant. So, I decided to add more variables to the initial database to check if the p-values would change.

### 3) What Factors Predict User Adoption?
"""

X = os_data_X.drop([], axis=1)
y = os_data_y
model = XGBClassifier()
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)
#print(X.shape)
#print(y.shape)
n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

X = os_data_X
y = os_data_y
logit = sm.Logit(y, X)

result = logit.fit()
result.summary()

"""##### 3a) Writeup associated with what factors predict user adoption?

### 4) Additional Commentary (Optional)

With more time, I would have liked to explore the correlation between some of the variables in the data provided to eliminate any bias in my methodology. From the initial scatter plots, it seems as if variable X is correlated with variable Y.

```
# This is formatted as code
```
"""